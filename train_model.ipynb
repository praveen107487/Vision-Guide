{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "max_length = 50  # Maximum length of captions\n",
    "vocab_size = 10000  # Vocabulary size\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO dataset (assuming you have the dataset downloaded)\n",
    "# For simplicity, this is a placeholder. Replace with actual data loading.\n",
    "def load_coco_data(data_dir):\n",
    "    # Placeholder for loading images and captions\n",
    "    # In practice, use pycocotools or similar\n",
    "    # images = []  # List of image features (e.g., from CNN)\n",
    "    # captions = []  # List of tokenized captions\n",
    "    # Example: Load from annotations.json\n",
    "    with open(os.path.join(data_dir, 'annotations/captions_train2017.json'), 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Extract captions and image paths\n",
    "    captions_data = annotations['annotations']\n",
    "    images_data = annotations['images']\n",
    "    image_id_to_path = {img['id']: img['file_name'] for img in images_data}\n",
    "    \n",
    "    captions = []\n",
    "    images = []\n",
    "    for ann in captions_data:\n",
    "        caption = ann['caption']\n",
    "        image_id = ann['image_id']\n",
    "        image_path = os.path.join(data_dir, 'train2017', image_id_to_path[image_id])\n",
    "        if os.path.exists(image_path):\n",
    "            images.append(image_path)\n",
    "            captions.append(caption)\n",
    "    \n",
    "    return images, captions\n",
    "\n",
    "# Preprocess images (extract features using a CNN)\n",
    "def preprocess_images(image_paths):\n",
    "    model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).resize((299, 299))\n",
    "        img = np.array(img) / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        feature = model.predict(img)\n",
    "        features.append(feature.flatten())\n",
    "    return np.array(features)\n",
    "\n",
    "# Tokenize captions\n",
    "def tokenize_captions(captions, max_length, vocab_size):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    sequences = tokenizer.texts_to_sequences(captions)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return tokenizer, padded_sequences\n",
    "\n",
    "# Define the model\n",
    "def create_model(vocab_size, embedding_dim, max_length, units):\n",
    "    # Image feature input\n",
    "    image_input = Input(shape=(2048,))  # Assuming InceptionV3 features\n",
    "    image_dense = Dense(units, activation='relu')(image_input)\n",
    "    image_dropout = Dropout(0.5)(image_dense)\n",
    "    \n",
    "    # Caption input\n",
    "    caption_input = Input(shape=(max_length,))\n",
    "    caption_embedding = Embedding(vocab_size, embedding_dim, input_length=max_length)(caption_input)\n",
    "    caption_lstm = LSTM(units, return_sequences=False)(caption_embedding)\n",
    "    caption_dropout = Dropout(0.5)(caption_lstm)\n",
    "    \n",
    "    # Combine\n",
    "    combined = concatenate([image_dropout, caption_dropout])\n",
    "    output = Dense(vocab_size, activation='softmax')(combined)\n",
    "    \n",
    "    model = Model(inputs=[image_input, caption_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Main training function\n",
    "def train_model(data_dir):\n",
    "    # Load data\n",
    "    image_paths, captions = load_coco_data(data_dir)\n",
    "    \n",
    "    # Preprocess\n",
    "    image_features = preprocess_images(image_paths[:1000])  # Subset for example\n",
    "    tokenizer, padded_captions = tokenize_captions(captions[:1000], max_length, vocab_size)\n",
    "    \n",
    "    # Prepare targets (shifted captions for training)\n",
    "    X_images = image_features\n",
    "    X_captions = padded_captions[:, :-1]\n",
    "    y = to_categorical(padded_captions[:, 1:], num_classes=vocab_size)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(vocab_size, embedding_dim, max_length-1, units)\n",
    "    \n",
    "    # Train\n",
    "    model.fit([X_images, X_captions], y, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model.save('image_caption_model.h5')\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Model trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training (replace 'path/to/coco' with actual path)\n",
    "train_model('path/to/coco')  # Update this with your COCO dataset path\n",
    "\n",
    "# Note: This is a basic example. Ensure you have the COCO dataset and install required packages: pip install tensorflow pillow numpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
